{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam80402/mywebsite/blob/main/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E6%9C%9F%E4%B8%AD%E5%A0%B1%E5%91%8A_%E6%9C%80%E7%B5%82%E6%A8%A1%E5%9E%8B%E7%A8%8B%E5%BC%8F%E7%A2%BCModel1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nydshy_nlvrC"
      },
      "source": [
        "# Model 2(使用tensorflow版本)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRXe0-18rPfA"
      },
      "source": [
        "### input train.csv / test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJvu2Q-9rPfB"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"fashion-mnist_train.csv\")\n",
        "data_test = pd.read_csv(\"fashion-mnist_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK_PcurnrPfB"
      },
      "source": [
        "data_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIrIClpTrPfC"
      },
      "source": [
        "unique_labels = data_train[\"label\"].unique()\n",
        "unique_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gulTMLYfrPfD"
      },
      "source": [
        "data_train.iloc[:,1:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkyIKrBSrPfE"
      },
      "source": [
        "import numpy as np\n",
        "np.reshape(data_train.iloc[:,1:].values[0], (28,28)).astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qMOZYwlrPfE"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 希望將 data reshape成 (28*28)，並將資料限縮在[0~1]之間\n",
        "def restructure_data(data):\n",
        "    return np.reshape(data.values,(28,28)).astype('float32')/ 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpJBniPLrPfF"
      },
      "source": [
        "cat_map= {0 : \"T-shirt/top\",\n",
        "          1 : \"Trouser\",\n",
        "          2 : \"Pullover\",\n",
        "          3 : \"Dress\",\n",
        "          4 : \"Coat\",\n",
        "          5 : \"Sandal\",\n",
        "          6 : \"Shirt\",\n",
        "          7 : \"Sneaker\",\n",
        "          8 : \"Bag\",\n",
        "          9 : \"Ankle boo\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSInpfwHrPfF"
      },
      "source": [
        "# 將 labels對應到 classs' name\n",
        "data_train[\"label\"] = data_train[\"label\"].map(lambda x : cat_map[x] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tZ2bg4arPfF"
      },
      "source": [
        "data_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xk4gMB5rPfF"
      },
      "source": [
        "# one-hot encoding\n",
        "\n",
        "dum = pd.get_dummies(data_train[\"label\"])\n",
        "dum.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQDjwl7drPfG"
      },
      "source": [
        "len(dum) #總共有6000個 labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP0W9XySrPfG"
      },
      "source": [
        "dum['Ankle boo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kE2xMtfrPfH"
      },
      "source": [
        "for col in dum.columns: # ['Ankle boo', 'Bag', 'Coat']...\n",
        "    data_train[col] = dum[col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbwA7NSLrPfI"
      },
      "source": [
        "data_train.columns[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmisODSVrPfI"
      },
      "source": [
        "# 原本的columns: \"labels\"(1) + \"pixel 1~pixel 784\"(784) + \"10個dum.columns\"(10) = 795\n",
        "len(data_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRLF-mesrPfK"
      },
      "source": [
        "y = data_train.iloc[:,-10:].values \n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-y2lvQArPfL"
      },
      "source": [
        "# prepare data for training \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def gen():\n",
        "    batch = data_train\n",
        "    \n",
        "    # x: input training picture / 把每一橫行打造成(28*28)後進行標準化\n",
        "    x = batch.iloc[:,1:-10].apply(restructure_data,axis =1).values \n",
        "    \n",
        "    # y: correct label\n",
        "    y = batch.iloc[:,-10:].values \n",
        "    \n",
        "    for x_item,y_item in zip(x,y):\n",
        "        yield(x_item,y_item)\n",
        "\n",
        "# 可以逐批讀取資料，不必一次將資料全部讀取放在記憶體\n",
        "def get_input_data(batch_size = 10):\n",
        "    return tf.data.Dataset.from_generator(gen,\n",
        "                                          output_signature=(tf.TensorSpec(shape=(28, 28), dtype=tf.float32),\n",
        "                                                            tf.TensorSpec(shape=(10), dtype=tf.float32))).shuffle(batch_size).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOa1U-vmrPfM"
      },
      "source": [
        "# input_shape 指出输入的形状(rows,cols,channels)\n",
        "input_layer = tf.keras.layers.Input(shape=(28,28,1))\n",
        "\n",
        "h = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(input_layer)\n",
        "\n",
        "h = tf.keras.layers.MaxPool2D()(h)\n",
        "\n",
        "h = tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu')(h)\n",
        "\n",
        "h = tf.keras.layers.MaxPool2D()(h)\n",
        "\n",
        "h = tf.keras.layers.Flatten()(h)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(10,activation= \"softmax\")(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zAKSMZOrPfN"
      },
      "source": [
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "batch_size = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmaxy3osrPfN"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53oN9YuJrPfO"
      },
      "source": [
        "# %%bash\n",
        "# mkdir -p tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBwhX5dTrPfP"
      },
      "source": [
        "# plot model graph\n",
        "# tf.keras.utils.plot_model(model, to_file=\"./tmp/model.png\", show_shapes=True,show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVr6IxNxrPfQ"
      },
      "source": [
        "class KerasCustomModel(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            \n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        \n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        \n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        \n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t7qk4U9rPfQ"
      },
      "source": [
        "keras_custom_model = KerasCustomModel(input_layer,output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm_stCe_rPfQ"
      },
      "source": [
        "# 對模型進行設置，損失函數、優化器、指標\n",
        "\n",
        "keras_custom_model.compile(optimizer=\"adam\", \n",
        "                           loss=\"categorical_crossentropy\", \n",
        "                           metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2uV3o7drPfR"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# start_time = datetime.now()\n",
        "\n",
        "# 訓練\n",
        "keras_custom_model.fit(get_input_data(), epochs=10, verbose=1)\n",
        "\n",
        "# end_time = datetime.now()\n",
        "# time_elapsed = end_time - start_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mq0ZZOlleB6"
      },
      "source": [
        "# Model1最終版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRxpb2idmOAQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf8B5QJ-Nktk"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h-AV-BsmhJL"
      },
      "source": [
        "def load_mnist(path, kind='train'):\n",
        "    import os\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "\n",
        "    \"\"\"Load MNIST data from `path`\"\"\"\n",
        "    labels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "    images_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\n",
        "    with gzip.open(labels_path, 'rb') as lbpath:\n",
        "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "    with gzip.open(images_path, 'rb') as imgpath:\n",
        "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io3BKAE4o_pl"
      },
      "source": [
        "X_train, y_train = load_mnist('drive/MyDrive/人工智慧導論/data/fashion', kind='train')\n",
        "X_test, y_test = load_mnist('drive/MyDrive/人工智慧導論/data/fashion', kind='t10k')\n",
        "\n",
        "# --輸入資料的標準化--\n",
        "ave_trainX = np.average(X_train)\n",
        "ave_testX = np.average(X_test)\n",
        "std_trainX = np.std(X_train)\n",
        "std_testX = np.std(X_test)\n",
        "input_train = (X_train - ave_trainX) / std_trainX\n",
        "input_test = (X_test - ave_testX) / std_testX\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_train_onehot = np_utils.to_categorical(y_train)\n",
        "y_test_onehot = np_utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_hK4FLupBuh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "correct_train = y_train_onehot  # 正確答案\n",
        "correct_test = y_test_onehot  #  正確答案\n",
        "\n",
        "n_train = input_train.shape[0]  # 訓練資料的樣本數 (75)\n",
        "n_test = input_test.shape[0]  # 測試資料的樣本數 (75)\n",
        "\n",
        "# -- 各設定値 --\n",
        "n_in = 784  # 輸入層的神經元數量\n",
        "n_mid = 512  # 中間層的神經元數量\n",
        "n_out = 10  # 輸出層的神經元數量\n",
        "\n",
        "wb_width = 0.01  # 設定權重參數的初始值乘上 0.01\n",
        "eta = 0.005  # 學習率 (eta)\n",
        "epoch = 25\n",
        "batch_size = 100\n",
        "interval = 10\n",
        "\n",
        "# -- 父類別 --\n",
        "class BaseLayer:\n",
        "  def __init__(self, n_upper, n):\n",
        "    # print('權重:',np.random.randn(n_upper, n))\n",
        "    # print('偏值:',np.random.randn(n))\n",
        "    self.w = wb_width * np.random.randn(n_upper, n)  # 權重矩陣\n",
        "    self.b = wb_width * np.random.randn(n)  # 偏值向量\n",
        "\n",
        "    self.h_w = np.zeros((n_upper, n )) + 1e-8\n",
        "    self.h_b = np.zeros(n) + 1e-8\n",
        "        \n",
        "  def update(self, eta):\n",
        "    # self.w -= eta * self.grad_w\n",
        "    # self.b -= eta * self.grad_b\n",
        "\n",
        "    self.h_w += self.grad_w * self.grad_w\n",
        "    self.w -= eta / np.sqrt(self.h_w) * self.grad_w\n",
        "\n",
        "    self.h_b += self.grad_b * self.grad_b\n",
        "    self.b -= eta / np.sqrt(self.h_b) * self.grad_b\n",
        "\n",
        "# -- Dropout layer --\n",
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio):\n",
        "    self.dropout_ratio = dropout_ratio #丟棄率\n",
        "\n",
        "  def forward( self, x, is_train): \n",
        "    if is_train:\n",
        "      rand = np.random.rand(*x.shape) #亂數的矩陣\n",
        "      self.dropout = np.where(rand > self.dropout_ratio, 1 ,0)\n",
        "      self.y = x * self.dropout\n",
        "    else:\n",
        "      self.y = (1-self.dropout_ratio)*x\n",
        "\n",
        "  def backward(self,grad_y):\n",
        "    self.grad_x = grad_y * self.dropout\n",
        "\n",
        "\n",
        "# -- 中間層 --\n",
        "class MiddleLayer(BaseLayer):\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    self.u = np.dot(x, self.w) + self.b\n",
        "    self.y = np.where(self.u <= 0, 0, self.u) # ReLU\n",
        "    # self.y = np.where(self.u <= 0, 0.01*self.u, self.u) # Leaky ReLU\n",
        "    # self.y = 1/(1+np.exp(-self.u) # Sigmoid \n",
        "\n",
        "  def backward(self, grad_y):\n",
        "    # delta = grad_y * self.u(1- self.u)  # Sigmoid 的微分\n",
        "    delta = grad_y * np.where(self.u <= 0, 0, 1)  # ReLU的微分\n",
        "    # delta = grad_y * np.where(self.u <= 0, 0.01, 1)  # Leaky ReLU的微分\n",
        "    self.grad_w = np.dot(self.x.T, delta)\n",
        "    self.grad_b = np.sum(delta, axis=0)\n",
        "    self.grad_x = np.dot(delta, self.w.T) \n",
        "\n",
        "# -- 輸出層 --\n",
        "class OutputLayer(BaseLayer):     \n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    u = np.dot(x, self.w) + self.b\n",
        "    self.y = np.exp(u)/np.sum(np.exp(u), axis=1, keepdims=True)  # Softmax\n",
        "\n",
        "  def backward(self, t):\n",
        "    delta = self.y - t\n",
        "    self.grad_w = np.dot(self.x.T, delta)\n",
        "    self.grad_b = np.sum(delta, axis=0)\n",
        "    self.grad_x = np.dot(delta, self.w.T) \n",
        "\n",
        "# -- 各層的實體化 --\n",
        "middle_layer_1 = MiddleLayer(n_in, n_mid)\n",
        "dropout_1 = Dropout(0.5)\n",
        "middle_layer_2 = MiddleLayer(n_mid, n_mid)\n",
        "dropout_2 = Dropout(0.5)\n",
        "output_layer = OutputLayer(n_mid, n_out)\n",
        "\n",
        "# -- 前向傳播 --\n",
        "def forward_propagation(x,is_train):\n",
        "  middle_layer_1.forward(x)\n",
        "  dropout_1.forward(middle_layer_1.y ,is_train)\n",
        "  middle_layer_2.forward(middle_layer_1.y)\n",
        "  dropout_2.forward(middle_layer_2.y ,is_train)\n",
        "  output_layer.forward(middle_layer_2.y)\n",
        "\n",
        "# -- 反向傳播 --\n",
        "def backpropagation(t):\n",
        "  output_layer.backward(t)\n",
        "  dropout_2.backward(output_layer.grad_x)\n",
        "  middle_layer_2.backward(output_layer.grad_x)\n",
        "  dropout_1.backward(middle_layer_2.grad_x)\n",
        "  middle_layer_1.backward(middle_layer_2.grad_x)\n",
        "\n",
        "# -- 修正權重參數 --\n",
        "def update_wb():\n",
        "  middle_layer_1.update(eta)\n",
        "  middle_layer_2.update(eta)\n",
        "  output_layer.update(eta)\n",
        "\n",
        "# -- 計算誤差 --\n",
        "def get_error(t, batch_size):\n",
        "  return -np.sum(t * np.log(output_layer.y + 1e-7)) / batch_size  # 交叉熵誤差\n",
        "\n",
        "# -- 開始訓練 --\n",
        "# -- 記錄誤差用 --\n",
        "train_error_x = []\n",
        "train_error_y = []\n",
        "test_error_x = []\n",
        "test_error_y = []\n",
        "\n",
        "# -- 記錄學習與進度--\n",
        "n_batch = n_train // batch_size  # 每 1 epoch 的批次數量\n",
        "\n",
        "for i in range(epoch):\n",
        "  # -- 計算誤差 --  \n",
        "  forward_propagation(input_train ,False)\n",
        "  error_train = get_error(correct_train, n_train)\n",
        "  forward_propagation(input_test, False)\n",
        "  error_test = get_error(correct_test, n_test)\n",
        "    \n",
        "  # -- 記錄誤差 -- \n",
        "  test_error_x.append(i)\n",
        "  test_error_y.append(error_test) \n",
        "  train_error_x.append(i)\n",
        "  train_error_y.append(error_train) \n",
        "    \n",
        "  # -- 顯示進度 -- \n",
        "  if i%interval == 0:\n",
        "    print(\"Epoch:\" + str(i) + \"/\" + str(epoch),\n",
        "       \"Error_train:\" + str(error_train),\n",
        "       \"Error_test:\" + str(error_test))\n",
        "   \n",
        "  # -- 訓練 -- \n",
        "  index_random = np.arange(n_train)\n",
        "  np.random.shuffle(index_random)  # 索引洗牌\n",
        "  for j in range(n_batch):\n",
        "        \n",
        "    # 取出小批次\n",
        "    mb_index = index_random[j*batch_size : (j+1)*batch_size]\n",
        "    x = input_train[mb_index, :]\n",
        "    t = correct_train[mb_index, :]\n",
        "        \n",
        "    # 前向傳播與反向傳播\n",
        "    forward_propagation(x ,True)\n",
        "    backpropagation(t)\n",
        "       \n",
        "    # 更新權重與偏值\n",
        "    update_wb()\n",
        "\n",
        "# -- 以圖表顯示誤差記錄 -- \n",
        "plt.plot(train_error_x, train_error_y, label=\"Train\")\n",
        "plt.plot(test_error_x, test_error_y, label=\"Test\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()\n",
        "\n",
        "# -- 計算準確率 -- \n",
        "forward_propagation(input_train,True)\n",
        "count_train = np.sum(np.argmax(output_layer.y, axis=1) == np.argmax(correct_train, axis=1))\n",
        "\n",
        "forward_propagation(input_test,True)\n",
        "count_test = np.sum(np.argmax(output_layer.y, axis=1) == np.argmax(correct_test, axis=1))\n",
        "\n",
        "print(\"Accuracy Train:\", str(count_train/n_train*100) + \"%\",\n",
        "      \"Accuracy Test:\", str(count_test/n_test*100) + \"%\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Shk8LGuRz5L"
      },
      "source": [
        "middle_layer_1,dropout_1,middle_layer_2,dropout_2,output_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLNvhqSTrwS"
      },
      "source": [
        "class model():\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.drop1 = None\n",
        "    self.middle = None\n",
        "    self.drop2 = None\n",
        "    self.output = None\n",
        "  \n",
        "  def save(self,Input,drop1,middle,drop2,output):\n",
        "    self.input = Input\n",
        "    self.drop1 = drop1\n",
        "    self.middle = middle\n",
        "    self.drop2 = drop2\n",
        "    self.output = output\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRRyZ7YMVxqJ"
      },
      "source": [
        "Model = model()\n",
        "Model.save(middle_layer_1,dropout_1,middle_layer_2,dropout_2,output_layer)\n",
        "import pickle\n",
        "with open('drive/MyDrive/人工智慧導論/Model.pickle', 'wb') as f:\n",
        "  pickle.dump(Model,f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}